package token

import (
	"strings"
)

type TokenType string
type TokenCat string

const (
	IDENT TokenType = "IDENT"

	ILLEGAL = "ILLEGAL"
	EOF     = "EOF"

	INT       = "Int"    // 1343456
	FLOAT     = "Float"  // 3.42
	STRING    = "String" // contents between " or """
	RAWSTRING = "RAWSTRING"

	// GQL Input Values types
	FUNC   = "func"
	FILTER = "filter"

	// Operators
	COLON  = ":"
	COMMA  = ","
	ATSIGN = "@"

	// Boolean operators

	AND = "AND"
	OR  = "OR"
	NOT = "NOT"

	TRUE  = "true"
	FALSE = "false"

	LPAREN   = "("
	RPAREN   = ")"
	LBRACE   = "{"
	RBRACE   = "}"
	LBRACKET = "["
	RBRACKET = "]"

	EXPAND = "..."
	// delimiters
	RAWSTRINGDEL = `"""`

	STRINGDEL = `"`

	BOM = "BOM"

	// Functions
	RFUNC = "RF"
	EQ    = "eq"
	LT    = "lt"
	HAS   = "has"
	ANY   = "anyofterms"
	ALL   = "allofterms"
	// modifiers
	MFUNC = "MF"
	COUNT = "count"
	VAL   = "val"
)

type Pos struct {
	Line int
	Col  int
}

// Token is exposed via token package so lexer can create new instanes of this type as required.
type Token struct {
	Type    TokenType
	Literal string // string value of token - rune, string, int, float, bool
	Loc     Pos    // start position of token
	Illegal bool
}

var keywords = map[string]struct {
	Type TokenType
}{
	"id":     {ID},
	"and":    {AND},
	"or":     {OR},
	"not":    {NOT},
	"filter": {FILTER},
	"true":   {TRUE},
	"false":  {FALSE},
	"func":   {FUNC},
	// suppored functions
	EQ:  {RFUNC},
	LT:  {RFUNC},
	HAS: {RFUNC},
	ANY: {RFUNC},
	ALL: {RFUNC},
	// supported modifer funcs
	COUNT: {MFUNC},
	VAL:   {MFUNC},
}

func LookupIdent(ident string) TokenType {
	if tok, ok := keywords[strings.ToLower(ident)]; ok {
		return tok.Type
	}
	return IDENT
}
